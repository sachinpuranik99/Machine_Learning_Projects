{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering on Netflix Dataset(50 points)\n",
    "\n",
    "In this part, collaborative filtering on the Netflix prize dataset is implemented\n",
    "\n",
    "Algorithm from the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset: Unique movies = 1821 and unique users = 28978 and total ratings = 3255352\n",
      "Testset: Unique movies = 1701 and unique users = 27555 and total ratings = 100478\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of ratings, \n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"netflix-dataset/TrainingRatings.txt\", sep=\",\", header=None, names = ('movie', 'user', 'rating'))\n",
    "test_data = pd.read_csv(\"netflix-dataset/TestingRatings.txt\", sep=\",\", header=None, names = ('movie', 'user', 'rating'))\n",
    "\n",
    "print \"Trainset: Unique movies =\", train_data.groupby(['movie']).ngroups, \"and unique users =\", train_data.groupby(['user']).ngroups, \"and total ratings =\", train_data['rating'].count()\n",
    "print \"Testset: Unique movies =\", test_data.groupby(['movie']).ngroups,\"and unique users =\", test_data.groupby(['user']).ngroups, \"and total ratings =\", test_data['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the user movie pivot table for trainset\n",
    "def GetPivotTable(file_handle):\n",
    "    user_movie_array = []\n",
    "    \n",
    "    # Parsing the values in the file and appending to user_movie_array\n",
    "    for line in file_handle:\n",
    "        values = []\n",
    "        split_arr = line.rstrip().split(\",\")\n",
    "        user_movie_array.append([int(split_arr[1]), int(split_arr[0]), float(split_arr[2])])\n",
    "        \n",
    "    # Converting to numpy type for optimization\n",
    "    user_movie_array = np.array(user_movie_array)\n",
    "    \n",
    "    # rows contain unique rows in increasing order. row_pos contains,\n",
    "    # for each element in the original array whats the correspngind position in the new aray\n",
    "    # For ex: 1744889 corresponds to 9, so => 9th position in the matrix. So, if we want to extract user for 1st row,\n",
    "    # get the value from the first element of the row and then index into row to find the user.\n",
    "    rows, row_pos = np.unique(user_movie_array[:, 0], return_inverse=True)\n",
    "    #print rows, row_pos, len(rows), len(row_pos)\n",
    "    \n",
    "    cols, col_pos = np.unique(user_movie_array[:, 1], return_inverse=True)\n",
    "    \n",
    "    # Creating a pivot table type structure which is in a form of a matrix\n",
    "    pivot_table = np.zeros((len(rows), len(cols)), dtype='float')\n",
    "    pivot_table[row_pos, col_pos] = user_movie_array[:, 2]\n",
    "\n",
    "    return pivot_table, rows, row_pos, cols, col_pos\n",
    "\n",
    "# Function to get the user movie pivot table for testset, structureally same as above function but breaks after 5000 unique users\n",
    "def GetPivotTableTest(file_handle):\n",
    "    user_movie_array = []\n",
    "    \n",
    "    # Creating a set to get first 5000 unique users\n",
    "    user_set = set()\n",
    "    \n",
    "    # Creating a set to get first 5000 unique users\n",
    "    user_set = set()\n",
    "    \n",
    "    \n",
    "    for line in file_handle:\n",
    "        values = []\n",
    "        split_arr = line.rstrip().split(\",\")\n",
    "        if int(split_arr[1]) not in user_set:\n",
    "            user_set.add(int(split_arr[1]))\n",
    "        user_movie_array.append([int(split_arr[1]), int(split_arr[0]), float(split_arr[2])])\n",
    "            \n",
    "        if(len(user_set) == 5000):\n",
    "            break\n",
    "                \n",
    "\n",
    "    user_movie_array = np.array(user_movie_array)\n",
    "    # rows contain unique rows in increasing order. row_pos contains,\n",
    "    # for each element in the original array whats the correspngind position in the new aray\n",
    "    # For ex: 1744889 corresponds to 9, so => 9th position in the matrix. So, if we want to extract user for 1st row,\n",
    "    # get the value from the first element of the row and then index into row to find the user.\n",
    "    rows, row_pos = np.unique(user_movie_array[:, 0], return_inverse=True)\n",
    "    #print rows, row_pos, len(rows), len(row_pos)\n",
    "    \n",
    "    cols, col_pos = np.unique(user_movie_array[:, 1], return_inverse=True)\n",
    "\n",
    "    pivot_table = np.zeros((len(rows), len(cols)), dtype='float')\n",
    "    pivot_table[row_pos, col_pos] = user_movie_array[:, 2]\n",
    "\n",
    "    return pivot_table, rows, row_pos, cols, col_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28978L, 1821L) (5000L, 124L)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "train_data_f = open(\"netflix-dataset/TrainingRatings.txt\", 'r')\n",
    "test_data_f = open(\"netflix-dataset/TestingRatings.txt\", 'r')\n",
    "#test_data_f = open(\"netflix-dataset/TestingSmall.txt\", 'r')\n",
    "\n",
    "# Creating train test datastructures\n",
    "user_movie_train_dict = {}\n",
    "user_train_array = []\n",
    "movie_train_array = []\n",
    "user_movie_test_dict = {}\n",
    "user_test_array = []\n",
    "movie_test_array = []\n",
    "\n",
    "# Getting the pivot table and data structures from the functions defined above\n",
    "user_train_array, user_id_train_row, user_id_train_row_index_pos, movie_train_col, movie_train_col_index_pos  = GetPivotTable(train_data_f)\n",
    "user_test_actual_array, user_id_test_row, user_id_test_row_index_pos, movie_test_col, movie_test_col_index_pos = GetPivotTableTest(test_data_f)\n",
    "user_test_predict_array = np.zeros((len(user_id_test_row), len(movie_test_col)), dtype='float')\n",
    "\n",
    "print user_train_array.shape, user_test_actual_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.90384615]\n",
      " [3.63095238]\n",
      " [3.94366197]\n",
      " ...\n",
      " [3.7721519 ]\n",
      " [3.85185185]\n",
      " [2.98076923]]\n"
     ]
    }
   ],
   "source": [
    "# I am taking k as per this formula\n",
    "# https://en.wikipedia.org/wiki/Collaborative_filtering#Memory-based \n",
    "k = 0.0001\n",
    "\n",
    "# Memoization for mean vote for user i\n",
    "# Computing the average\n",
    "Vi = np.true_divide(user_train_array.sum(1),(user_train_array!=0).sum(1))\n",
    "Vi  = Vi.reshape(len(user_train_array),1)\n",
    "# Converting to a numpy array\n",
    "np.array(Vi)\n",
    "# Printing sample values\n",
    "print Vi\n",
    "\n",
    "# Memoization for correlation factor Vij and Vij_square. So, that compuatation is efficient.\n",
    "Vij = np.copy(user_train_array[: , :])\n",
    "\n",
    "# Mask for keeping the matrix sparse.\n",
    "Vij_mask = (Vij == 0) \n",
    "# Subtracting the mean\n",
    "Vij -= Vi\n",
    "# Sparsing operation\n",
    "Vij[Vij_mask] = 0\n",
    "# Square operation\n",
    "Vij_sqr = np.square(Vij)\n",
    "\n",
    "# Sum of Vij_square for denominator of equation 2\n",
    "Vij_sum = np.sum(Vij_sqr, axis=1)\n",
    "#print Vij_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get mean rating for a particular user\n",
    "def GetUserVa(user_id):\n",
    "    return Vi[np.where(user_id_train_row == user_id)]\n",
    "\n",
    "# Computing the correlation. I am vectorizing the whole operation for better speed.\n",
    "def correlation(user_id, Vij, Vij_sum):\n",
    "    # Current user data\n",
    "    Va = user_train_array[np.where(user_id_train_row == user_id), :]\n",
    "    Va = np.array(Va)\n",
    "    Va = Va.reshape(1, len(movie_train_col))\n",
    "    \n",
    "    # Subtracting the mean and keeping the matrix sparse\n",
    "    Va[Va != 0] -= float(GetUserVa(user_id))\n",
    "    \n",
    "    # Numerator of equation 2\n",
    "    numerator = np.dot(Va, Vij.T)\n",
    "    \n",
    "    # First half of the denominator\n",
    "    Va_sqr = np.square(Va)\n",
    "    \n",
    "    # Mask for multiplying only common non-zero entries\n",
    "    mask = np.logical_or(Va_sqr == 0, Vij_sqr == 0)\n",
    "    # Broadcasting array for vectorization\n",
    "    Va_sqr = [Va_sqr]*len(user_id_train_row)\n",
    "    # Masking the non-common zeros\n",
    "    Va_sqr = np.ma.masked_array(Va_sqr, mask=mask)\n",
    "    # Reshaping \n",
    "    Va_sqr = Va_sqr.reshape(Vij_sqr.shape)\n",
    "    # Va_sqr sum for denominator\n",
    "    Va_sqr_sum = np.sum(Va_sqr, axis=1)\n",
    "    # Reshaping to make it look similar if it is not already.\n",
    "    Vij_sum = Vij_sum.reshape(Va_sqr_sum.shape)\n",
    "    \n",
    "    # Denominator computaition and numpy array conversiion equation (2) of the paper\n",
    "    denominator = np.sqrt(Va_sqr_sum*Vij_sum)\n",
    "    denominator = np.array(denominator)\n",
    "    \n",
    "    return np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n",
    "\n",
    "\n",
    "def SumOfProduct(user_id, movie, Vij, Vij_sum):\n",
    "    # Getting the output of equation (2)\n",
    "    w_a_i = correlation(user_id, Vij, Vij_sum)\n",
    "    \n",
    "    # Computing vij - vi  equation (1) considering nonzero weights only.\n",
    "    vij_norm = np.copy(user_train_array[:,np.where(movie_train_col == movie)])\n",
    "    vij_norm = vij_norm.reshape(Vi.shape)\n",
    "    vij_norm_mask = (vij_norm == 0)\n",
    "    vij_norm -= Vi\n",
    "    vij_norm[vij_norm_mask] = 0\n",
    "    \n",
    "    # Computing the summation part of equation (1)\n",
    "    sop  = np.dot(w_a_i, vij_norm)\n",
    "    \n",
    "    return sop, w_a_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Softwares\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "E:\\Softwares\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Time is 3795.33100009\n"
     ]
    }
   ],
   "source": [
    "done = 0\n",
    "\n",
    "# Profiling the run-time\n",
    "import time\n",
    "tic = time.time()\n",
    "\n",
    "# Computing the actual ratings by doing the computation prescribed in (1) and (2)\n",
    "for idx in range(0,len(user_id_test_row)):\n",
    "    for idy in range(0,len(movie_test_col)):\n",
    "        # No need to compute for non rated items\n",
    "        if(user_test_actual_array[idx][idy] != 0):\n",
    "            # Get mean\n",
    "            Va_final = GetUserVa(user_id_test_row[idx])\n",
    "            # Get correlation for k computation and second half of equation (1)\n",
    "            sop, w_a_i = SumOfProduct(user_id_test_row[idx], movie_test_col[idy], Vij, Vij_sum)\n",
    "            # Compute k as per https://en.wikipedia.org/wiki/Collaborative_filtering#Memory-based\n",
    "            k_mult_sop = (1/np.sum(np.abs(w_a_i)))*sop\n",
    "            # Store the prediction in predict array\n",
    "            user_test_predict_array[idx][idy] =  Va_final + k_mult_sop\n",
    "            done += 1\n",
    "            if (done % 1000 == 0):\n",
    "                print done \n",
    "            \n",
    "toc = time.time()\n",
    "\n",
    "print \"Time is\", toc-tic\n",
    "\n",
    "#print user_test_predict_array, user_test_actual_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for first 5000 unique users is\n",
      "0.7591577735646583\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "print \"Mean Absolute Error for first 5000 unique users is\"\n",
    "print np.nansum(np.abs(user_test_actual_array-user_test_predict_array))/np.count_nonzero(user_test_predict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error for first 5000 unique users is\n",
      "0.9609789112221285\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error\n",
    "print \"Root Mean Squared Error for first 5000 unique users is\"\n",
    "print np.sqrt(np.nansum(np.square(user_test_actual_array-user_test_predict_array))/np.count_nonzero(user_test_predict_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in /home/sachin/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numpy>=1.11.2 in /home/sachin/anaconda2/lib/python2.7/site-packages (from scikit-surprise)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/sachin/anaconda2/lib/python2.7/site-packages (from scikit-surprise)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/sachin/anaconda2/lib/python2.7/site-packages (from scikit-surprise)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing surprise package for the svd implementation\n",
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fb2af39d990>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, dataset, SVD, SVDpp, NMF,  evaluate, accuracy\n",
    "\n",
    "# Reading the Training ratings\n",
    "df1 = pd.read_csv(\"netflix-dataset/TrainingRatings.txt\", sep=\",\", header=None, names = ('itemID', 'userID', 'rating'))\n",
    "\n",
    "# Using the reader object to convert the pandas dataframe to surprise frame\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(df1[['userID', 'itemID', 'rating']], reader)\n",
    "\n",
    "# Using SVD: matrix factorization approach \n",
    "algo = SVD(n_epochs = 25, verbose = False)\n",
    "\n",
    "# Building the raw trainset from the converted pandas frame\n",
    "trainset = data.build_full_trainset() \n",
    "\n",
    "# Training the SVD \n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for first 5000 unique users using SVD is: \n",
      "MAE:  0.6614\n",
      "Root Mean Squared Error for first 5000 unique users using SVD is: \n",
      "RMSE: 0.8542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8541781352993629"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the test set as pandas frame\n",
    "df2 = pd.read_csv(\"netflix-dataset/TestingRatings.txt\", sep=\",\", header=None, names = ('itemID', 'userID', 'rating'))\n",
    "\n",
    "# Removing the duplicate users so that only first 5000 users are picked\n",
    "df2 = df2.loc[~df2['userID'].duplicated()]\n",
    "\n",
    "# Selecting first 5000 user specific data frame\n",
    "df2 = df2.iloc[:5000,:]\n",
    "\n",
    "# Converting the pandas data frame \n",
    "test_data = Dataset.load_from_df(df2[['userID', 'itemID', 'rating']], reader)\n",
    "\n",
    "# Building the full testset\n",
    "testset_buf = test_data.build_full_trainset() \n",
    "# Converting the testset to test specific format ie masking the ratings so that its reserved for RMSE/MAE computation\n",
    "testset = testset_buf.build_testset()\n",
    "\n",
    "# Running the predict method to get the true ratings.\n",
    "predictions = algo.test(testset)\n",
    "# Passing the predictions returned by the test method to RMSE and MAE computation functions\n",
    "print \"Mean Absolute Error for first 5000 unique users using SVD is: \"\n",
    "accuracy.mae(predictions, verbose=True)\n",
    "print \"Root Mean Squared Error for first 5000 unique users using SVD is: \"\n",
    "accuracy.rmse(predictions, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for RMSE/MAE improvement\n",
    "\n",
    "1. I am using the famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.\n",
    "https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\n",
    "\n",
    "2. Since the matrix factorization technique for sparse matrix is an optimization problem I am using prebuild optimization solvers provided by the surprise package.\n",
    "3. I am using SGD solver of SVD package and keeping the number of epochs as 25 and number of factors also as 20\n",
    "4. Learning rates are set to 0.005 and regularization terms are set to 0.02.\n",
    "5. I also tried some enhancements in the https://arxiv.org/pdf/1301.7363.pdf paper. But the results were not that great. I had also tried NMF approach mentioned here https://ieeexplore.ieee.org/document/6748996/, but the results were not that great. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
